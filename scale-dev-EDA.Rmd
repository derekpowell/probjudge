---
title: "scale dev EDA"
output: html_notebook
---


```{r}
# 2021-06-16
library(tidyverse)
df <- read_csv("data/pilot-scaledev.csv")
```

```{r}
df %>% 
  group_by(trial_num, type, ID) %>% 
  summarize(
    sd = sd(pj, na.rm=TRUE)
  ) %>% 
  ggplot(aes(x = sd, fill = type, color=type)) +
  geom_density(alpha=.33) +
  facet_wrap(~trial_num)
```

Slider looks the best here, but maybe not hugely better than the likert scale. Quite clear that my "guided" idea is actually the worst. 

In the plots below, you get more extremely variant responses on certain kinds of trials. Graph below is lazy but gets the gist probably: slider and likert are similar. Based on this, averaging three trials should be a good guard (as I figured from the paper). Averaging two is probably not too bad either, though I'm not sure. Using individual responses appears hopelessly doomed however (as I also expected), at least without accounting for the contaminant trials.  

Responses are NOT getting worse over blocks, so it's not just that people are bored/fatigued.

```{r}
df %>% 
  group_by(trial_num, ID) %>% 
  # filter(conf>0) %>%
  mutate(
    pj_mean = mean(pj, na.rm=TRUE),
    diff = abs(pj - pj_mean)
    ) %>% 
  ggplot(aes(x=diff, fill=type)) +
  geom_histogram(alpha=.5)

## by item/trial
df %>% 
  group_by(trial_num, ID) %>% 
  # filter(conf>0) %>%
  mutate(
    pj_mean = mean(pj, na.rm=TRUE),
    diff = abs(pj - pj_mean)
    ) %>% 
  group_by(type, trial_num) %>% 
  summarize(
    over20percent = mean(ifelse(diff > .20, 1, 0), na.rm=TRUE),
    sd = sd(ifelse(diff> .20, 1, 0), na.rm=TRUE)/sqrt(n()) # lazy sd approx, no good in this range
    # over25percent = mean(ifelse(diff > .25, 1, 0), na.rm=TRUE),
    # over30percent = mean(ifelse(diff > .30, 1, 0), na.rm=TRUE)
  ) %>% 
  ggplot(aes(x = reorder(trial_num, over20percent), color=type, y = over20percent, ymin = over20percent-sd, ymax=over20percent+sd)) + geom_pointrange(position=position_dodge(width=.5))

## across blocks
df %>% 
  group_by(trial_num, ID) %>% 
  # filter(conf>0) %>%
  mutate(
    pj_mean = mean(pj, na.rm=TRUE),
    diff = abs(pj - pj_mean)
    ) %>% 
  mutate(
    block = ordered(block, levels = c("b1","b2","b3"))
  ) %>% 
  group_by(type, block) %>% 
  summarize(
    over20percent = mean(ifelse(diff > .20, 1, 0), na.rm=TRUE),
    sd = sd(ifelse(diff> .20, 1, 0), na.rm=TRUE)/sqrt(n()) # lazy sd approx, no good in this range
    # over25percent = mean(ifelse(diff > .25, 1, 0), na.rm=TRUE),
    # over30percent = mean(ifelse(diff > .30, 1, 0), na.rm=TRUE)
  ) %>% 
  ggplot(aes(x = block, color=type, y = over20percent, ymin = over20percent-sd, ymax=over20percent+sd)) + geom_pointrange(position=position_dodge(width=.5))
```

Just in terms of raw histograms of responses, they really look pretty similar!

```{r}
df %>% 
  ggplot(aes(x=pj, color=type, fill=type)) +
  geom_density( alpha=.2) +
  facet_wrap(~trial_num) +
  theme_bw()

df %>% 
  ggplot(aes(x=pj, color=type, fill=type)) +
  geom_histogram(bins=21, alpha=1) +
  facet_wrap(~type) +
  theme_bw()
```

How do respondents feel using the response modalities? Looks like they like the likert and slider more.

```{r}
df %>% 
  group_by(ID, type) %>% 
  summarize(mean_conf = mean(conf, na.rm=TRUE)) %>% 
  ggplot(aes(x=type, y = mean_conf)) +
  geom_boxplot()
```
Is their confidence indicative of anything? Yes slightly indicative but not hugely

```{r}

df %>% 
  group_by(trial_num, ID, type) %>% 
  # filter(conf>0) %>%
  mutate(
    pj_mean = mean(pj, na.rm=TRUE),
    absdiff = abs(pj - pj_mean)
    ) %>% 
  mutate(
    is_conf = ifelse(conf>0, "yes", "no")
  ) %>% 
  group_by(is_conf, type) %>% 
  summarize(
    M = mean(absdiff, na.rm=TRUE),
    se = sd(absdiff, na.rm=TRUE)/sqrt(n())
  ) %>%
  ggplot(aes(x=is_conf, y = M, ymin= M-se, ymax = M+se)) +
  geom_pointrange() +
  facet_wrap(~type)


df %>% 
  group_by(trial_num, ID, type) %>% 
  # filter(conf>0) %>%
  mutate(
    pj_mean = mean(pj, na.rm=TRUE),
    absdiff = abs(pj - pj_mean)
    ) %>% 
  summarize(
    mean_conf = mean(conf, na.rm=TRUE),
    mean_diff = mean(absdiff, na.rm=TRUE)
  ) %>% 
  ggplot(aes(x=mean_conf, y = mean_diff)) +
  geom_jitter() +
  geom_smooth()
  # ggplot(aes(x = factor(conf), y = absdiff)) +
  # geom_boxplot()
```


How similar are the trial-average responses under the different response modalities? Very similar!

```{r}
df %>% 
  group_by(trial_num, type) %>% 
  # filter(conf>0) %>%
  summarize(
    pj_mean = mean(pj, na.rm=TRUE),
    ) %>%
  spread(type, pj_mean) %>% 
  ungroup() %>% 
  select(-trial_num) %>% 
  pairs()
  # ggplot(aes(x = sd,fill = type, color=type)) +
  # geom_density(alpha=.33) +
  # facet_wrap(~trial_num)
```
## in sum

Seems like all 3 would be legitimate ways to measure probability judgments (at least, they are all highly correlated with one another on average). However, the "guided" approach is clearly awful. Likert percentage choices and sliders seem equally good in most respects. The "confidence" responses are not entirely unrelated to variance, but they are not informative enough on an individual level to be at all useful.  So confidence follow-up questions and guided approaches are clearly both dead-ends.

Could use a probability likert scale followed with a slider scale and then average them. If separated, they would provide a brain-fart outlier check. And it could reduce the need to account for rounding, simplifying the model. No silver bullet in here though unfortunately.


